{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Model Comparison & Selection\n",
    "\n",
    "In this task, we will compare the performance of different transformer-based models for Amharic NER, such as XLM-Roberta, mBERT, and DistilBERT. We will fine-tune each model on our labeled dataset, evaluate their performance, and select the best model for EthioMart's business needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "We will compare the following models:\n",
    "- XLM-Roberta (`xlm-roberta-base`)\n",
    "- Multilingual BERT (`bert-base-multilingual-cased`)\n",
    "- DistilBERT (`distilbert-base-multilingual-cased`)\n",
    "\n",
    "Each model will be fine-tuned and evaluated using the same data and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = {\n",
    "    \"XLM-Roberta\": \"xlm-roberta-base\",\n",
    "    \"mBERT\": \"bert-base-multilingual-cased\",\n",
    "    \"DistilBERT\": \"distilbert-base-multilingual-cased\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We will reuse the labeled dataset and preprocessing pipeline from Task 3. The data will be loaded, tokenized, and split into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_path = \"../data/labeled/ner_labeled_sample.conll\"\n",
    "label_list = [\"O\", \"B-Product\", \"I-Product\", \"B-LOC\", \"I-LOC\", \"B-PRICE\", \"I-PRICE\"]\n",
    "label_to_id = {l: i for i, l in enumerate(label_list)}\n",
    "id_to_label = {i: l for l, i in label_to_id.items()}\n",
    "\n",
    "def read_conll(path):\n",
    "    tokens, labels, sentences = [], [], []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append({\"tokens\": tokens, \"ner_tags\": labels})\n",
    "                    tokens, labels = [], []\n",
    "            else:\n",
    "                splits = line.split()\n",
    "                tokens.append(splits[0])\n",
    "                labels.append(splits[1])\n",
    "        if tokens:\n",
    "            sentences.append({\"tokens\": tokens, \"ner_tags\": labels})\n",
    "    return sentences\n",
    "\n",
    "def encode_labels(example):\n",
    "    example[\"labels\"] = [label_to_id[l] for l in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "def tokenize_and_align_labels(example, tokenizer):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    labels = []\n",
    "    previous_word_idx = None\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(example[\"labels\"][word_idx])\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "data = read_conll(conll_path)\n",
    "from datasets import Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset = dataset.map(encode_labels)\n",
    "split = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "raw_train_dataset = split[\"train\"]\n",
    "raw_eval_dataset = split[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation\n",
    "\n",
    "We will fine-tune and evaluate each model using the same training and validation data. For each model, we will:\n",
    "- Load the pre-trained model and tokenizer\n",
    "- Tokenize and align the data\n",
    "- Fine-tune the model\n",
    "- Evaluate its performance (F1-score, precision, recall)\n",
    "- Record the results for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    "import numpy as np\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "results = {}\n",
    "\n",
    "for model_name, model_checkpoint in model_names.items():\n",
    "    print(f\"\\n=== Training {model_name} ===\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    train_dataset = raw_train_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer), batched=False)\n",
    "    eval_dataset = raw_eval_dataset.map(lambda x: tokenize_and_align_labels(x, tokenizer), batched=False)\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_checkpoint, num_labels=len(label_list), id2label=id_to_label, label2id=label_to_id\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f\"./results/{model_name}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=8,\n",
    "        per_device_eval_batch_size=8,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"./logs/{model_name}\",\n",
    "        logging_steps=10,\n",
    "        save_total_limit=2,\n",
    "        report_to=\"none\"\n",
    "    )\n",
    "    \n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    \n",
    "    predictions, labels, _ = trainer.predict(eval_dataset)\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "    true_labels = [[id_to_label[l] for l in label if l != -100] for label in labels]\n",
    "    pred_labels = [[id_to_label[p] for (p, l) in zip(pred, label) if l != -100] for pred, label in zip(preds, labels)]\n",
    "    \n",
    "    report = classification_report(true_labels, pred_labels, output_dict=True)\n",
    "    results[model_name] = {\n",
    "        \"f1\": f1_score(true_labels, pred_labels),\n",
    "        \"report\": report\n",
    "    }\n",
    "    print(f\"\\n{model_name} F1-score: {results[model_name]['f1']:.4f}\")\n",
    "    print(classification_report(true_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "We summarize the F1-scores and other metrics for each model to compare their performance and select the best model for our NER task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "summary = {model: {\"F1-score\": res[\"f1\"]} for model, res in results.items()}\n",
    "df = pd.DataFrame(summary).T\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
