{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: FinTech Vendor Scorecard for Micro-Lending\n",
    "\n",
    "In this task, we will analyze vendor activity and engagement on Telegram e-commerce channels using the entities extracted by our NER model and available metadata (e.g., views, timestamps). We will compute key business metrics for each vendor and design a simple \"Lending Score\" to help EthioMart identify promising vendors for micro-lending."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "We will load the processed message data (with metadata) and the NER predictions for each message. This will allow us to aggregate statistics per vendor/channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "# Load all messages from all channels\n",
    "data_dir = \"data/processed/text\"\n",
    "all_files = []\n",
    "for channel in os.listdir(data_dir):\n",
    "    channel_dir = os.path.join(data_dir, channel)\n",
    "    if os.path.isdir(channel_dir):\n",
    "        all_files.extend(glob(os.path.join(channel_dir, \"*.json\")))\n",
    "\n",
    "messages = []\n",
    "for file_path in all_files:\n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        msg = json.load(f)\n",
    "        msg['channel'] = os.path.basename(os.path.dirname(file_path))\n",
    "        messages.append(msg)\n",
    "\n",
    "df = pd.DataFrame(messages)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run NER Model on Messages\n",
    "\n",
    "We will use our fine-tuned NER model to extract entities (Product, Price, Location) from each message. The results will be used to compute business metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_path = \"./finetuned-ner-model\"  # Update if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    try:\n",
    "        ents = ner_pipeline(text)\n",
    "        products = [ent['word'] for ent in ents if ent['entity_group'] == 'Product']\n",
    "        prices = [ent['word'] for ent in ents if ent['entity_group'] == 'PRICE']\n",
    "        locations = [ent['word'] for ent in ents if ent['entity_group'] == 'LOC']\n",
    "        return products, prices, locations\n",
    "    except Exception as e:\n",
    "        return [], [], []\n",
    "\n",
    "df['products'], df['prices'], df['locations'] = zip(*df['text'].map(extract_entities))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Vendor Metrics\n",
    "\n",
    "We will calculate the following metrics for each vendor/channel:\n",
    "- Posting Frequency (posts per week)\n",
    "- Average Views per Post\n",
    "- Top Performing Post (highest views, product, price)\n",
    "- Average Price Point\n",
    "- Lending Score (custom weighted score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Posting frequency (posts per week)\n",
    "vendor_stats = df.groupby('channel').agg(\n",
    "    posts_per_week = ('timestamp', lambda x: x.count() / ((x.max() - x.min()).days / 7 + 1)),\n",
    "    avg_views = ('views', 'mean'),\n",
    "    top_post_views = ('views', 'max'),\n",
    "    avg_price = ('prices', lambda x: np.mean([float(p) for sublist in x for p in sublist if str(p).replace('.','',1).isdigit()]) if any(x) else np.nan)\n",
    ").reset_index()\n",
    "\n",
    "# Top performing post info\n",
    "top_posts = df.loc[df.groupby('channel')['views'].idxmax()][['channel', 'text', 'products', 'prices', 'views']].rename(\n",
    "    columns={'text': 'top_post_text', 'products': 'top_post_products', 'prices': 'top_post_prices', 'views': 'top_post_views'}\n",
    ")\n",
    "vendor_stats = vendor_stats.merge(top_posts, on=['channel', 'top_post_views'])\n",
    "\n",
    "# Lending Score (example: 0.5*avg_views + 0.5*posts_per_week)\n",
    "vendor_stats['lending_score'] = 0.5 * vendor_stats['avg_views'] + 0.5 * vendor_stats['posts_per_week']\n",
    "\n",
    "vendor_stats = vendor_stats.sort_values('lending_score', ascending=False)\n",
    "vendor_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vendor Scorecard\n",
    "\n",
    "Below is the summary table comparing vendors on key metrics and the final Lending Score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_cols = [\n",
    "    'channel', 'avg_views', 'posts_per_week', 'avg_price', 'lending_score',\n",
    "    'top_post_text', 'top_post_products', 'top_post_prices', 'top_post_views'\n",
    "]\n",
    "vendor_stats[display_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "The Vendor Scorecard provides actionable insights for EthioMart to identify the most active and promising vendors for micro-lending. The Lending Score combines engagement and activity metrics, helping prioritize vendors for financial support."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
